# -*- coding: utf-8 -*-
"""carbon_infer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RlqS0eLyTgr70FEvGMjrpayVuc5eC3ET
"""
import os
# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install tzlocal!=3 # Required to resolve an error with codecarbo. Must be placed before you install codecarbon too
# !pip install codecarbon datasets
# !pip install git+https://github.com/huggingface/transformers
# !pip install sentencepiece # Required to resolve this error: https://github.com/huggingface/transformers/issues/9750
# # If using colab, restart your runtime after this cell finishes (don't factory reset it)

os.system('sudo apt install sysfsutils')

os.system('sudo chmod -R a+r /sys/class/powercap/intel-rapl')

"""# Introduction

This notebooks collects a rough estimate of the emissions generated during inference of a transformer language model. Testing both GPU & CPU inference emissions.

This version is specifically written to be compatible with Colab. For other platforms check out our repo: https://github.com/bigscience-workshop/carbon-footprint

Model being tested: https://huggingface.co/bigscience/T0 (Currently testing the script with the 3B model since we only have one GPU)


## Eval Tasks
https://huggingface.co/bigscience/T0#evaluation-data
- Natural Language Inference
- Coreference Resolution
- Word sense disambiguation
- Sentence completion

## Batch Sizes

- 16
- 32
- 64
- 128
- 256
- 512
- 1024

## Token Sizes
- Small: TBD
- Medium: TBD
- Large: TBD

## Methodology



"""

# Get the CPU we're using
os.system("lscpu | egrep 'Model name|Socket|Thread|NUMA|CPU\(s\)'")

# Get the GPU we're using
os.system('nvidia-smi -L')

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from codecarbon import EmissionsTracker
from datasets import load_dataset
from datetime import datetime

# Print the start time of the script
print(f"Script starting: {datetime.now()}")

isGpu = torch.cuda.is_available()
device = "cuda:0" if isGpu else "cpu"

# Establish constants
BATCH_SIZES = [16, 32, 64, 128, 256, 512, 1024]
MEASURE_INTERVAL = 1
RUNS = 1 #
DEFAULT_MODEL = "hf-internal-testing/tiny-random-t5" # TODO: This is only 3B while testing on a single GPU
device

tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)
model = AutoModelForSeq2SeqLM.from_pretrained(DEFAULT_MODEL)
model = model.to(device)

# Timestamp when we've loaded the model to make sure we don't count that in emissions
print(f"Model loaded: {datetime.now()}")

print(f"Begin dataset loading: {datetime.now()}")
nli_dataset = load_dataset("anli")
coref_dataset = load_dataset("winograd_wsc", 'wsc285')
word_sense_dataset = load_dataset("anli")
sentence_dataset = load_dataset("anli")
print(f"Dataset loading complete: {datetime.now()}")

def create_batch(query: str, batch_size: int):
  return [query for i in range(batch_size)]

def export_emissions_csv(emissions, project_name: str):
  pass

def run_model(query: str, project_name: str):
  """ """
  emissions = []
  for batch in [16]:
    queries = create_batch(query, batch)
    tracker = EmissionsTracker(project_name=f"{project_name}_batch_size_{batch}", measure_power_secs=MEASURE_INTERVAL)
    tracker.start()
    for i in range(RUNS):
      for q in queries:
        inputs = tokenizer.encode(q, return_tensors="pt").to(device)
        outputs = model.generate(inputs)
    emissions.append(tracker.stop())
  return emissions

query = "Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. In the previous sentence, decide who 'her' is referring to."

run_model(query, "carbonprompt")

#
